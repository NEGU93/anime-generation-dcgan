{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "# Deep Convolutional Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network\n",
    "\n",
    "Las gan se basan en un punto de vista de teoría de juegos. Al contrario de una red neuronal convencional. En las gan se utilizan dos redes neuronales que reciben el nombre de **Generador** y **Discriminador**. Dichas redes están en constante competencia entre ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] Generative Adversarial Networks\n",
    "##### Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n",
    "\n",
    "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GAN Diagram](img/1_XKanAdkjQbg1eDDMF2-4ow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta distribución, la red generadora intenta generar imágenes que logren engañar al discriminador haciendole creer que las imágenes son originales. El discriminador por su parte trata de no ser engañado e intenta distinguir si las imágenes fueron originales o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El generador recibe como entrada un vector de numeros aleatorios (que llamaremos z) y lo transforma en datos que desea imitar.\n",
    "\n",
    "El discriminador recibo como input tanto los datos reales (x) como los generados por el generador (G(z)) y computa la probabilidad de que esa entrada sea real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function\n",
    "\n",
    "El discriminador trata de maximizar la función (gradient ascent) mientras que el discriminador trata de minimizarla (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T22:42:09.297309Z",
     "start_time": "2018-08-11T22:42:09.267330Z"
    }
   },
   "source": [
    "![Objective Function](img/1_FbQLpEVQKsMSK-c7_5KcWw.png)\n",
    "\n",
    "En la siguiente imágen, el primer término corresponde a que los datos reales tengan un alto valor mientras que motiva a los datos generados G(z) que sean ranqueados con una probabilidad baja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.D, self.D_logits = self.discriminator(inputs, self.y, reuse=False)\n",
    "self.D_, self.D_logits_ = self.discriminator(self.G, self.y, reuse=True)\n",
    "\n",
    "# p * -tf.log(q) + (1 - p) * -tf.log(1 - q), q es el primer parámetro y p el segundo\n",
    "self.d_loss_real = tf.reduce_mean(sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))\n",
    "self.d_loss_fake = tf.reduce_mean(sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n",
    "\n",
    "self.d_loss = self.d_loss_real + self.d_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, al aplicar esta ecuación, el generador no funciona tan bien. Esto sucede porque cuando una imagen es generada es probable que lo clasifique como falso. El gradiende tiende a ser bastante plano y dificulta que el modelo aprenda correctamente. Por dicho motivo se cambia la función del generador por la siguiente:\n",
    "\n",
    "![Generator Objective Function](img/1_ZHKnky7Pzi5OvPlUIZhDjg.png)\n",
    "\n",
    "Es decir, que en lugar de minimizar la probabilidad de que el discriminador tenga razón, maximiza el *likelyhood* de que el discriminador se equivoque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.g_loss = tf.reduce_mean(sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generador\n",
    "\n",
    "![Generator](img/gernerator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entrada del generador es una entrada aleatoria denominada *latent sample*. El generador toma esa entrada y la convierte en la imagen generada.\n",
    "\n",
    "Resulta evidente que sin entrenamiento, la salida de la red será ruido sin significado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminador \n",
    "\n",
    "El discriminador recibe una imágen y dice si la misma fue real (1) o no (0).\n",
    "\n",
    "![Generator](img/disc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T23:36:09.964480Z",
     "start_time": "2018-08-11T23:36:09.955486Z"
    }
   },
   "source": [
    "### Entrenamiento\n",
    "    1. Poner el discriminador en modo de entrenamiento\n",
    "    2. Entrenar el discriminador tanto con imágenes generadas como reales\n",
    "    3. Poner el discriminador en modo de no entrenamiento\n",
    "    4. Entrenar el generador como parte de la GAN\n",
    "    5. Repetir paso 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A tener en cuenta\n",
    "\n",
    "Si el discriminador entrena mucho más rápido que el generador, el generador nunca logra engañar al discriminador. Lo mismo aplica para el otro caso en donde el discriminador termina no pudiendo clasificar apropiadamente. Se debe tener cuidado para lograr que ambos logren entrenarse a un ritmo similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "CNNs son especialmente útiles para clasificación y reconocimiento de imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN poseen a grandes rasgos dos componentes principales:\n",
    "    1. Las capas ocultas (feature extraction)\n",
    "    2. Clasificación\n",
    "    \n",
    "![Generator](img/1_NQQiyYqJJj4PSYAeWvxutg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "En este componente se realizan operaciones de **convolucion** y **pooling** en las cuales los patrones son detectados.\n",
    "\n",
    "Si se buscara reconocer una zebra por ejemplo, esta etapa reconocería las rayas, dos oídos y cuatro patas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolución\n",
    "\n",
    "En la convolución se dice que se convoluciona la imagen de entrada con un **kernel** o **filtro** para generar un **feature map**. Para realizar la convolución se mueve el filtro sobre la imagen de entrada multiplicando y sumando el resultado en el *feature map*. \n",
    "\n",
    "En la siguiente imágen peude observarse claramente cómo se realiza dicha operación.\n",
    "![conv](img/1_VVvdh-BUKFh2pwDD0kPeRA@2x.gif)\n",
    "\n",
    "En la práctica se realizan numerosas convoluciones sobre la entrada usando diferentes filtros. Esto genera numerosos *feature maps* los cuales se los junta para obtener la salida final de la capa de convolución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de activación\n",
    "\n",
    "Como en cualquier otra red neuronal, se usa una **función de activación** para que la salida sea no lineal. Por ejemplo la función ReLU (Rectified Linear Units - https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)\n",
    "\n",
    "$$ f(x) = max(x, 0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stride\n",
    "\n",
    "Stride se le llama al *paso* (cantidad de pixels) que el filtro debe moverse a cada iteración. Usualmente es 1. Aumentando dicho número puede reducirse el overlap.\n",
    "\n",
    "![stride](img/0_iqNdZWyNeCr5tCkc_.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "El tamaño del *feature map* es SIEMPRE menor que el input. Es por eso que se debe usar **padding**.\n",
    "\n",
    "Una capa de pixels nulos (valor cero) se agrega al input, rodeando al mismo de ceros y aumentando de esa forma su tamaño. De esta forma se logra que no se reduzca el *feature map*. El ejemplo de stride superior incluye un padding representado por los cuadrados de linea punteada.\n",
    "\n",
    "El padding además, mejora la performance y se asegura que el tamaño del kernel y del stride sean coherentes con la entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling\n",
    "\n",
    "Luego de una capa de convolución, es común agregar una capa de **pooling**. Su función es reducir continuamente las dimensiones reduciendo la complejidad de la red.\n",
    "\n",
    "Lo mismo decrementa el tiempo de entrenamiento y reduce el overfitting.\n",
    "\n",
    "##### Max Pooling\n",
    "\n",
    "El modo más común de pooling se llama **max pooling** el cual toma el máximo valor de cada ventana. En la siguiente figura se muestra un ejemplo de max pooling:\n",
    "\n",
    "![stride](img/1_vbfPq-HvBCkAcZhiSTZybg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumen\n",
    "\n",
    "Al usar una CNN hay 4 hiperparámetros importantes entre los cuales decidir:\n",
    "\n",
    "1. Kernel size\n",
    "2. Filter count (cuantos filtros usar)\n",
    "3. Stride\n",
    "4. Padding\n",
    "\n",
    "Visualización de una capa convolucional:\n",
    "\n",
    "![stride](img/1__34EtrgYk6cQxlJ2br51HQ.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Ocurre luego de las capas de convolución y pooling.\n",
    "\n",
    "Clasifica como una red convencional sobre los patrones obtenidos.\n",
    "\n",
    "La parte de clasificación simplemente consiste en una red fully connected convirtiendo la matriz 3D (o 2D si es grayscale) en un vector 1D.\n",
    "\n",
    "La red se entrena igual que cualquier otra red, usando backpropagation / gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN\n",
    "\n",
    "![Generator](img/1_39Nnni_nhPDaLu9AnTLoWw.png)\n",
    "https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
    "##### Alec Radford, Luke Metz, Soumith Chintala\n",
    "\n",
    "In recent years, supervised learning with **convolutional networks (CNNs)** has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the **gap between the success of CNNs for supervised learning and unsupervised learning.** We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código\n",
    "\n",
    "La mayor parte del código se basó en https://github.com/carpedm20/DCGAN-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py\n",
    "\n",
    "Se importan los parámetros mediante la linea de comando gracias al módulo *flags* de TensorFlow. \n",
    "\n",
    "De esta forma se permite que un usuario utilice el programa de un modo sencillo sin necesidad de abrir ni modificar el código.\n",
    "\n",
    "**Nota:** Hay un bug en el código, es necesario usar la opción crop obligatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T17:24:46.118142Z",
     "start_time": "2018-08-12T17:24:46.090196Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-62d9d2a60afb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_json\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_all_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"epoch\", 25, \"Epoch to train [25]\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.0002, \"Learning rate of for adam [0.0002]\")\n",
    "flags.DEFINE_float(\"beta1\", 0.5, \"Momentum term of adam [0.5]\")\n",
    "flags.DEFINE_float(\"train_size\", np.inf, \"The size of train images [np.inf]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 64, \"The size of batch images [64]\")\n",
    "flags.DEFINE_integer(\"input_height\", 108, \"The size of image to use (will be center cropped). [108]\")\n",
    "flags.DEFINE_integer(\"input_width\", None,\n",
    "                     \"The size of image to use (will be center cropped). If None, same value as input_height [None]\")\n",
    "flags.DEFINE_integer(\"output_height\", 64, \"The size of the output images to produce [64]\")\n",
    "flags.DEFINE_integer(\"output_width\", None,\n",
    "                     \"The size of the output images to produce. If None, same value as output_height [None]\")\n",
    "flags.DEFINE_string(\"dataset\", \"anime-faces\", \"The name of dataset [celebA, mnist, lsun]\")\n",
    "flags.DEFINE_string(\"input_fname_pattern\", \"*.jpg\", \"Glob pattern of filename of input images [*]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"../data\", \"Root directory of dataset [data]\")\n",
    "flags.DEFINE_string(\"sample_dir\", \"samples\", \"Directory name to save the image samples [samples]\")\n",
    "flags.DEFINE_boolean(\"train\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"crop\", True, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"visualize\", False, \"True for visualizing, False for nothing [False]\")\n",
    "flags.DEFINE_integer(\"generate_test_images\", 100, \"Number of images to generate during test. [100]\")\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    pp.pprint(flags.FLAGS.__flags)                  # Muestro los parámetros\n",
    "\n",
    "    if FLAGS.input_width is None:\n",
    "        FLAGS.input_width = FLAGS.input_height      # Si no se especifica se asume imagen cuadrada\n",
    "    if FLAGS.output_width is None:\n",
    "        FLAGS.output_width = FLAGS.output_height\n",
    "\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):    # Crea las carpetas si no existen\n",
    "        os.makedirs(FLAGS.checkpoint_dir)\n",
    "    if not os.path.exists(FLAGS.sample_dir):\n",
    "        os.makedirs(FLAGS.sample_dir)\n",
    "\n",
    "    run_config = tf.ConfigProto()\n",
    "    run_config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=run_config) as sess:\n",
    "        dcgan = DCGAN(\n",
    "            sess,\n",
    "            input_width=FLAGS.input_width,\n",
    "            input_height=FLAGS.input_height,\n",
    "            output_width=FLAGS.output_width,\n",
    "            output_height=FLAGS.output_height,\n",
    "            batch_size=FLAGS.batch_size,\n",
    "            sample_num=FLAGS.batch_size,\n",
    "            z_dim=FLAGS.generate_test_images,\n",
    "            dataset_name=FLAGS.dataset,\n",
    "            input_fname_pattern=FLAGS.input_fname_pattern,\n",
    "            crop=FLAGS.crop,\n",
    "            checkpoint_dir=FLAGS.checkpoint_dir,\n",
    "            sample_dir=FLAGS.sample_dir,\n",
    "            data_dir=FLAGS.data_dir\n",
    "        )\n",
    "        show_all_variables()\n",
    "\n",
    "        if FLAGS.train:\n",
    "            dcgan.train(FLAGS)\n",
    "        else:\n",
    "            if not dcgan.load(FLAGS.checkpoint_dir)[0]:\n",
    "                raise Exception(\"[!] Train a model first, then run test mode\")\n",
    "\n",
    "        # to_json(\"./web/js/layers.js\", [dcgan.h0_w, dcgan.h0_b, dcgan.g_bn0],\n",
    "        #                 [dcgan.h1_w, dcgan.h1_b, dcgan.g_bn1],\n",
    "        #                 [dcgan.h2_w, dcgan.h2_b, dcgan.g_bn2],\n",
    "        #                 [dcgan.h3_w, dcgan.h3_b, dcgan.g_bn3],\n",
    "        #                 [dcgan.h4_w, dcgan.h4_b, None])\n",
    "\n",
    "        # Below is codes for visualization\n",
    "        OPTION = 1\n",
    "        visualize(sess, dcgan, FLAGS, OPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py\n",
    "\n",
    "Métodos principales:\n",
    "\n",
    "#### 1. Constructores/Inicializadores\n",
    "\n",
    "*Init:*\n",
    "\n",
    "    1.  Guarda las variables dentro del objeto.\n",
    "    2.  Batch Normalization.\n",
    "    3.  Se fija si es grayscale o a color (para poner la cantidad de canales).\n",
    "    4.  Verifica que el batch_size sea menor a la longitud total de los datos.\n",
    "    5.  Llama a build model.\n",
    "\n",
    "*build_model:*\n",
    "\n",
    "    1.  Setea las dimensiones según la entrada *crop*.\n",
    "        a.  Si crop entonces usa la salida como tamaño de entrada y corta las imágenes desde el centro.\n",
    "    2.  Llama a los consctructores del generador y el discriminador.\n",
    "    3.  Llama a los consctuctores del sampler.\n",
    "    4.  Crea las funciones de costo para ambas redes.\n",
    "    5.  Utiliza trainable_variables y lo divide en generador y discriminador para tener la lista de las variables a entrenar.\n",
    "    6.  Inicializa un objeto de clase *Saver* para ir guardando los checkpoints.\n",
    "\n",
    "*discriminator:*\n",
    "    \n",
    "    -  Crea la esctructura de la red del discriminador\n",
    "\n",
    "*generator:*\n",
    "    \n",
    "    -  De forma análoga a discriminador, crea la estructura de la red\n",
    "    -  Utiliza deconvolución en lugar de convolución\n",
    "\n",
    "*sampler:*\n",
    "\n",
    "#### 2. Entrenamiento\n",
    "\n",
    "*train:*\n",
    "\n",
    "    -  Adam Optimizer (parámetros beta para el decay del momentum)\n",
    "    1.  Genera el ruido de entrada\n",
    "    2.  Toma la cantidad de imágenes correspondiente al batch size\n",
    "    3.  Intenta abrir un checkpoint\n",
    "    4.  Comienza las epochs\n",
    "        4.1.  Para cada epoch divide en test set según el batch size\n",
    "        4.2.  Entrena D\n",
    "        4.3.  Entrena G\n",
    "        4.4.  Entrena G again (para que D_loss no llegue a cero)\n",
    "    5.  Si hay que hacer un sample hace sample\n",
    "    6.  Si checkpoint entonces guarda el checkpoint\n",
    "        \n",
    "    \n",
    "\n",
    "#### 3. Checkpoint\n",
    "\n",
    "*model_dir:*\n",
    "\n",
    "    -  Obtiene el nombre de un directorio a utilizar para guardar el modelo según:\n",
    "        1.  Nombre del dataset\n",
    "        2.  batch_size\n",
    "        3.  dimensiones de las imágenes de salida\n",
    "        \n",
    "*save:*\n",
    "\n",
    "    -  Guarda en el directorio dado por checkpoint y \"model_dir\" la sesión (sess)\n",
    "\n",
    "*load:*\n",
    "\n",
    "    -  Abre la sesión guardada con \"save\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T17:33:48.646139Z",
     "start_time": "2018-08-12T17:33:48.510397Z"
    }
   },
   "source": [
    "## Resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posibles mejoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T19:51:59.126178Z",
     "start_time": "2018-08-13T19:51:59.117182Z"
    }
   },
   "source": [
    "### How to Train a GAN? Tips and tricks to make GANs work\n",
    "https://github.com/soumith/ganhacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "https://danbooru.donmai.us/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset no uniforme\n",
    "\n",
    "    - Blanco y negro\n",
    "    - Errores de recorte\n",
    "    - Perfiles\n",
    "    \n",
    "[3] Dice: \" [...] their (Danbooru) datasets suffer from high inter-image variance and noise. Due to the fact that image boards allow uploading of images highly different in style, domain, and quality [...]\"\n",
    "\n",
    "No solo utilizan una base de datos más homogenea sino que luego revisan todos los datos y eliminan un 4% de los falsos negativos (lo cual no se hizo en este trabajo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"img/dataset/danbooru_901019_ed9e65500490e35ea9d892eb6a998ffb.png\" alt=\"Drawing\" /> </td>\n",
    "    <td> <img src=\"img/dataset/danbooru_853258_6a2fae7e35d57e53db563434a1c550d1.png\" alt=\"Drawing\" /> </td>\n",
    "    <td> <img src=\"img/dataset/danbooru_843704_8c4e83901fa17a685541fd67e218f2b0.png\" alt=\"Drawing\" /> </td>\n",
    "    \n",
    "    <td> <img src=\"img/dataset/danbooru_836273_d6147d08cf5521d625e729d93715b0fe.png\" alt=\"Drawing\" /> </td>\n",
    "    <td> <img src=\"img/dataset/danbooru_486418_f991370eb8779ed66e1aa8d5f0996395.jpg\" alt=\"Drawing\" /> </td>\n",
    "    <td> <img src=\"img/dataset/danbooru_454550_a81283d43c096498fdda8b332831a1b9.jpg\" alt=\"Drawing\" /> </td>\n",
    "    \n",
    "    <td> <img src=\"img/dataset/danbooru_444173_0066c72cc0066531084a288e1a4e45a7.jpg\" alt=\"Drawing\" /> </td>\n",
    "    <td> <img src=\"img/dataset/danbooru_440907_0b1a026392412dcfddab3871cb412bff.jpg\" alt=\"Drawing\" /> </td>\n",
    "    <td> <img src=\"img/dataset/danbooru_564654_5133896875a2d5952d035c573af0afb3.png\" alt=\"Drawing\" /> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía:\n",
    "\n",
    "#### [3] Towards the Automatic Anime Characters Creation with Generative Adversarial Networks\n",
    "\n",
    "Automatic generation of facial images has been well studied after the Generative\n",
    "Adversarial Network(GAN) came out. There exists some attempts applying the\n",
    "GAN model to the problem of generating facial images of anime characters, but\n",
    "none of the existing work gives a promising result. In this work, we explore the\n",
    "training of GAN models specialized on an anime facial image dataset. We address\n",
    "the issue from both the data and the model aspect, by collecting a more clean,\n",
    "well-suited dataset and leverage proper, empirical application of DRAGAN. With\n",
    "quantitative analysis and case studies we demonstrate that our efforts lead to a\n",
    "stable and high-quality model. Moreover, to assist people with anime character\n",
    "design, we build a website1 with our pre-trained model available online, which\n",
    "makes the model easily accessible to general public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "415px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "718px",
    "left": "0px",
    "right": "1324px",
    "top": "111px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
